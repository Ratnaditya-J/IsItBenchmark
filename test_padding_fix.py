#!/usr/bin/env python3
"""
Test script to validate the padding token fix for specialized model training.
This bypasses the complex import structure to directly test the core functionality.
"""

import torch
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    Trainer, TrainingArguments, DataCollatorWithPadding
)
from torch.utils.data import Dataset
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

class SimpleContaminationDataset(Dataset):
    """Simple dataset for testing padding token fix."""
    
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding=False,  # Dynamic padding via data collator
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def prepare_model_and_tokenizer(model_name="microsoft/DialoGPT-medium"):
    """Prepare model and tokenizer with proper padding token configuration."""
    
    print(f"Loading model and tokenizer: {model_name}")
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Add padding token if it doesn't exist (required for GPT-2 based models)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # Load model for sequence classification
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2,
        problem_type="single_label_classification",
        pad_token_id=tokenizer.pad_token_id  # Explicitly set pad_token_id in model config
    )
    
    # Resize token embeddings if needed
    model.resize_token_embeddings(len(tokenizer))
    
    # Ensure model config has correct padding token
    model.config.pad_token_id = tokenizer.pad_token_id
    
    # For GPT-2 based models, also set the model's pad_token_id attribute
    if hasattr(model, 'transformer') and hasattr(model.transformer, 'wte'):
        # This is a GPT-2 style model
        if not hasattr(model.config, 'pad_token_id') or model.config.pad_token_id is None:
            model.config.pad_token_id = tokenizer.pad_token_id
    
    print(f"Model loaded with {model.num_parameters():,} parameters")
    print(f"Tokenizer pad_token: {tokenizer.pad_token}")
    print(f"Tokenizer pad_token_id: {tokenizer.pad_token_id}")
    print(f"Model config pad_token_id: {model.config.pad_token_id}")
    
    return model, tokenizer

def compute_metrics(eval_pred):
    """Compute evaluation metrics."""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')
    accuracy = accuracy_score(labels, predictions)
    
    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

def test_padding_fix():
    """Test the padding token fix with a small training example."""
    
    print("="*60)
    print("TESTING PADDING TOKEN FIX FOR SPECIALIZED MODEL TRAINING")
    print("="*60)
    
    # Create sample data
    texts = [
        "What is the capital of France?",
        "How do you calculate the area of a circle?",
        "What is machine learning?",
        "Explain quantum computing.",
        "What is the speed of light?",
        "How does photosynthesis work?",
        "What is artificial intelligence?",
        "Explain the theory of relativity."
    ]
    
    labels = [1, 0, 1, 0, 1, 0, 1, 0]  # Alternating contaminated/clean
    
    print(f"Sample data: {len(texts)} texts, {sum(labels)} contaminated")
    
    # Prepare model and tokenizer
    try:
        model, tokenizer = prepare_model_and_tokenizer()
        print("‚úÖ Model and tokenizer preparation: SUCCESS")
    except Exception as e:
        print(f"‚ùå Model and tokenizer preparation: FAILED - {e}")
        return False
    
    # Create dataset
    try:
        dataset = SimpleContaminationDataset(texts, labels, tokenizer)
        print("‚úÖ Dataset creation: SUCCESS")
    except Exception as e:
        print(f"‚ùå Dataset creation: FAILED - {e}")
        return False
    
    # Create data collator
    try:
        data_collator = DataCollatorWithPadding(
            tokenizer=tokenizer,
            padding=True,
            return_tensors="pt"
        )
        print("‚úÖ Data collator creation: SUCCESS")
    except Exception as e:
        print(f"‚ùå Data collator creation: FAILED - {e}")
        return False
    
    # Test batch processing (this is where the padding error occurred)
    try:
        # Create a small batch
        batch_indices = [0, 1, 2]
        batch_data = [dataset[i] for i in batch_indices]
        
        # Test data collator
        collated_batch = data_collator(batch_data)
        
        print(f"‚úÖ Batch processing test: SUCCESS")
        print(f"   Batch size: {len(batch_data)}")
        print(f"   Input IDs shape: {collated_batch['input_ids'].shape}")
        print(f"   Attention mask shape: {collated_batch['attention_mask'].shape}")
        print(f"   Labels shape: {collated_batch['labels'].shape}")
        
    except Exception as e:
        print(f"‚ùå Batch processing test: FAILED - {e}")
        return False
    
    # Test model forward pass with batch
    try:
        model.eval()
        with torch.no_grad():
            outputs = model(**{k: v for k, v in collated_batch.items() if k != 'labels'})
            logits = outputs.logits
            
        print(f"‚úÖ Model forward pass: SUCCESS")
        print(f"   Output logits shape: {logits.shape}")
        
    except Exception as e:
        print(f"‚ùå Model forward pass: FAILED - {e}")
        return False
    
    # Test training setup (without actual training)
    try:
        training_args = TrainingArguments(
            output_dir="./test_output",
            num_train_epochs=1,
            per_device_train_batch_size=2,
            per_device_eval_batch_size=2,
            logging_steps=1,
            eval_strategy="no",
            save_strategy="no",
            report_to=[],
            remove_unused_columns=False
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=dataset,
            data_collator=data_collator,
            compute_metrics=compute_metrics
        )
        
        print("‚úÖ Trainer setup: SUCCESS")
        
    except Exception as e:
        print(f"‚ùå Trainer setup: FAILED - {e}")
        return False
    
    print("\n" + "="*60)
    print("üéâ PADDING TOKEN FIX VALIDATION: ALL TESTS PASSED!")
    print("="*60)
    print("The specialized model training pipeline is now ready.")
    print("The padding token configuration has been successfully fixed.")
    print("Batch processing with different sequence lengths now works correctly.")
    
    return True

if __name__ == "__main__":
    success = test_padding_fix()
    if success:
        print("\n‚úÖ Ready to proceed with full specialized model training!")
        exit(0)
    else:
        print("\n‚ùå Padding token fix validation failed!")
        exit(1)
